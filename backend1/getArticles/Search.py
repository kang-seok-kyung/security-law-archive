import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
import json
import time
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from datetime import datetime

# ===== ì„¤ì • =====
BASE_URL = "https://www.boannews.com"
START_URL = "https://www.boannews.com/search/news_hash.asp?find=%BB%E7%B0%C7%BB%E7%B0%ED"
HEADERS = {"User-Agent": "Mozilla/5.0"}
RAW_SAVE = "boan_news_raw.json"
SUMMARY_SAVE = "boan_news_summarized.json"

# ===== ì¶”ì • ê°€ëŠ¥í•œ í•œêµ­ ë²•ë¥  ëª©ë¡ =====
import torch

LAW_LIST = [
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•",
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ",
    "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•",
    "ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ",
    "ì „ìžì •ë¶€ë²•",
    "í†µì‹ ë¹„ë°€ë³´í˜¸ë²•",
    "ì „ìžê¸ˆìœµê±°ëž˜ë²•",
    "ì§€ëŠ¥ì •ë³´í™” ê¸°ë³¸ë²•",
    "ì „ìžë¬¸ì„œ ë° ì „ìžê±°ëž˜ ê¸°ë³¸ë²•",
    "êµ­ê°€ë³´ì•ˆë²•",
    "ë¶€ì •ê²½ìŸë°©ì§€ ë° ì˜ì—…ë¹„ë°€ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ",
    "êµ­ê°€ì •ë³´í™” ê¸°ë³¸ë²•",
    "ì „ìžì„œëª…ë²•"
]

# ì˜ˆì‹œ ê°€ì¤‘ì¹˜ ì‚¬ì „ (í‚¤ì›Œë“œ: {ë²•ë¥ : ê°€ì¤‘ì¹˜})
KEYWORD_WEIGHTS = {
    # ê°œì¸ì •ë³´ë³´í˜¸ë²•
    "ê°œì¸ì •ë³´": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.2},
    "ìœ ì¶œ": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15},
    "ì •ë³´": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.1},
    "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.2},
    "ì´ë¦„": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15},
    "ì£¼ì†Œ": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15},
    "íœ´ëŒ€ì „í™”ë²ˆí˜¸": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15},
    "ì˜ë£Œ ì •ë³´": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.1},

    # ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥  (ì •ë³´í†µì‹ ë§ë²•)
    "ë””ë„ìŠ¤": {"ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.25},
    "ë°”ì´ëŸ¬ìŠ¤": {"ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.20},
    "ì›¹ì‰˜": {"ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.15},
    "í•´í‚¹": {
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.30,
    "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.15,
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.10
    },
    "ì•…ì„±ì½”ë“œ": {
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.20,
    "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.15
    },
    "ëžœì„¬ì›¨ì–´": {
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.25,
    "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.15,
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.10
    },


    # ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•
    "ì •ë³´í†µì‹ ê¸°ë°˜": {"ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.30},
    "ì„œë²„ ìž¥ì• ": {"ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.20},
    "ë„¤íŠ¸ì›Œí¬ ìž¥ì• ": {"ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.20},

    # ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ 
    "ì‹ ìš©ì •ë³´": {"ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ": 0.30},
    "ì‹ ìš©ì¹´ë“œ": {"ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ": 0.20},
    "ëŒ€ì¶œì •ë³´": {"ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ": 0.20},

    # ì „ìžì •ë¶€ë²•
    "ì „ìžì •ë¶€": {"ì „ìžì •ë¶€ë²•": 0.30},
    "í–‰ì •ë§": {"ì „ìžì •ë¶€ë²•": 0.20},

    # í†µì‹ ë¹„ë°€ë³´í˜¸ë²•
    "ê°ì²­": {"í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.30},
    "ë„ì²­": {"í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.30},
    "í†µì‹  ê°ì²­": {"í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.25},

    # ì „ìžê¸ˆìœµê±°ëž˜ë²•
    "ì „ìžê¸ˆìœµ": {"ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.30},

    # ì§€ëŠ¥ì •ë³´í™” ê¸°ë³¸ë²•
    "ì¸ê³µì§€ëŠ¥": {"ì§€ëŠ¥ì •ë³´í™” ê¸°ë³¸ë²•": 0.30},
    "ë¹…ë°ì´í„°": {"ì§€ëŠ¥ì •ë³´í™” ê¸°ë³¸ë²•": 0.25},
    "ì§€ëŠ¥ì •ë³´": {"ì§€ëŠ¥ì •ë³´í™” ê¸°ë³¸ë²•": 0.20},

    # ì „ìžë¬¸ì„œ ë° ì „ìžê±°ëž˜ ê¸°ë³¸ë²•
    "ì „ìžë¬¸ì„œ": {"ì „ìžë¬¸ì„œ ë° ì „ìžê±°ëž˜ ê¸°ë³¸ë²•": 0.30},
    "ì „ìžê±°ëž˜": {"ì „ìžë¬¸ì„œ ë° ì „ìžê±°ëž˜ ê¸°ë³¸ë²•": 0.30},
    "ì˜¨ë¼ì¸ ê±°ëž˜": {"ì „ìžë¬¸ì„œ ë° ì „ìžê±°ëž˜ ê¸°ë³¸ë²•": 0.20},

    # êµ­ê°€ë³´ì•ˆë²•
    "êµ­ê°€ë³´ì•ˆ": {"êµ­ê°€ë³´ì•ˆë²•": 0.30},
    "êµ­ê°€ ê¸°ë°€": {"êµ­ê°€ë³´ì•ˆë²•": 0.25},
    "êµ­ê°€ì•ˆë³´": {"êµ­ê°€ë³´ì•ˆë²•": 0.20},

    # ë¶€ì •ê²½ìŸë°©ì§€ ë° ì˜ì—…ë¹„ë°€ë³´í˜¸ì— ê´€í•œ ë²•ë¥ 
    "ì˜ì—…ë¹„ë°€": {"ë¶€ì •ê²½ìŸë°©ì§€ ë° ì˜ì—…ë¹„ë°€ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ": 0.30},
    "ë¶€ì •ê²½ìŸ": {"ë¶€ì •ê²½ìŸë°©ì§€ ë° ì˜ì—…ë¹„ë°€ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ": 0.25},
    "ê¸°ìˆ  ìœ ì¶œ": {"ë¶€ì •ê²½ìŸë°©ì§€ ë° ì˜ì—…ë¹„ë°€ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ": 0.25},

    # êµ­ê°€ì •ë³´í™” ê¸°ë³¸ë²•
    "êµ­ê°€ì •ë³´í™”": {"êµ­ê°€ì •ë³´í™” ê¸°ë³¸ë²•": 0.30},
    "ê³µê³µê¸°ê´€ ì •ë³´": {"êµ­ê°€ì •ë³´í™” ê¸°ë³¸ë²•": 0.25},

    # ì „ìžì„œëª…ë²•
    "ì „ìžì„œëª…": {"ì „ìžì„œëª…ë²•": 0.30},
    "ì¸ì¦ì„œ": {"ì „ìžì„œëª…ë²•": 0.25},
    "ê³µì¸ì¸ì¦ì„œ": {"ì „ìžì„œëª…ë²•": 0.20},

    # ì¤‘ë³µ ë²•ë¥  ì ìš© ì˜ˆì‹œ (ì¶”ê°€)
    "ë°ì´í„° ìœ ì¶œ": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15, "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.15},
    "ì •ë³´ ë³´í˜¸": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.1, "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.1},
    "ë„¤íŠ¸ì›Œí¬ ê³µê²©": {"ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.25, "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.20},
    "ê³„ì¢Œì •ë³´": {"ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ": 0.20, "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.20},

    "ì‚¬ì´ë²„ ê³µê²©": {
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.30,
    "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.25
    },
    "ë”¥íŽ˜ì´í¬": {
        "ì§€ëŠ¥ì •ë³´í™” ê¸°ë³¸ë²•": 0.30,
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.20
    },
    "ê²°ì œ": {
        "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.30,
        "ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ": 0.25
    },
    "ì—…ë°ì´íŠ¸ ë¬¸ì œ": {
        "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.25,
        "ì „ìžì •ë¶€ë²•": 0.20
    },
    "DDoS": {
        "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.30
    },
    "ì•…ì„± ì•±": {
        "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.25,
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.20
    },
    "ê¸ˆìœµì •ë³´": {
        "ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ": 0.30,
        "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.25
    },
    "í”¼ì‹±": {
    "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.25,
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15,
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.15
    },
    "ìŠ¤ë¯¸ì‹±": {
    "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.20,
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15,
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.15
    },
    "ê³„ì¢Œ íƒˆì·¨": {
    "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.25,
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.10
    },
    "ê¸ˆìœµì‚¬ê¸°": {
    "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.25,
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.10
    },
    "ëª¸ìº ": {
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.25,
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.20,
    "ì „ìžê¸ˆìœµê±°ëž˜ë²•": 0.15
    },
    "ì˜ìƒ ìœ í¬": {
    "ì •ë³´í†µì‹ ë§ì´ìš©ì´‰ì§„ë°ì •ë³´ë³´í˜¸ë“±ì—ê´€í•œë²•ë¥ ": 0.25,
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15
    },
}

def apply_keyword_weights(text, base_probs):
    scores = base_probs.clone()
    text_lower = text.lower()
    
    triggered_keywords = set()

    for keyword, law_weights in KEYWORD_WEIGHTS.items():
        if keyword in text_lower:
            triggered_keywords.add(keyword)

    for keyword in triggered_keywords:
        for law, weight in KEYWORD_WEIGHTS[keyword].items():
            if law in LAW_LIST:
                idx = LAW_LIST.index(law)
                scores[idx] += weight

    total = scores.sum().item()
    if total > 0:
        scores /= total
    else:
        scores = base_probs

    return scores

def classify_laws(text, tokenizer, model, top_k=3, threshold=0.08, gap_threshold=0.05):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        base_probs = torch.softmax(outputs.logits, dim=1)[0]
    
    weighted_probs = apply_keyword_weights(text, base_probs)
    print("ìµœì¢… ê°€ì¤‘ì¹˜:", weighted_probs.tolist())

    # ìƒìœ„ top_k ì¸ë±ìŠ¤ì™€ í™•ë¥  ì¶”ì¶œ
    top_probs, top_indices = torch.topk(weighted_probs, top_k)

    # ìµœê³  í™•ë¥ ê°’ ê¸°ì¤€ìœ¼ë¡œ gap_threshold ì´í•˜ì¸ ë²•ë¥  ëª¨ë‘ í¬í•¨
    max_prob = top_probs[0].item()
    selected_laws = []
    for prob, idx in zip(top_probs, top_indices):
        if prob.item() >= threshold and (max_prob - prob.item()) <= gap_threshold:
            selected_laws.append((LAW_LIST[idx], prob.item()))

    if not selected_laws:
        return ["ë¯¸ë¶„ë¥˜"]
    
    # ë²•ë¥  ì´ë¦„ë§Œ ë°˜í™˜
    return [law for law, prob in selected_laws]

# ===== ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ =====
def get_article_list(page=1):
    url = f"{START_URL}&Page={page}"
    resp = requests.get(url, headers=HEADERS, verify=False)
    resp.encoding = 'euc-kr'
    soup = BeautifulSoup(resp.text, 'html.parser')
    articles = []
    for item in soup.select("div.news_list"):
        title = item.select_one("span.news_txt")
        date = item.select_one("span.news_writer")
        link = item.select_one("a[href^='/media/view.asp']")
        if title and date and link:
            # ì›ë³¸: "ê¸°ìžëª… | 2024ë…„ 10ì›” 28ì¼ 13:39"
            raw_date = date.get_text(strip=True).split('|')[-1].strip()
            try:
                parsed_date = datetime.strptime(raw_date, "%Yë…„ %mì›” %dì¼ %H:%M")
                formatted_date = parsed_date.strftime("%Y.%m.%d")
            except ValueError:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                formatted_date = raw_date
            articles.append({
                "title": title.get_text(strip=True),
                "date": formatted_date,
                "uri": urljoin(BASE_URL, link['href']),
            })
    return articles

# ===== ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘ =====
def get_article_content(url):
    try:
        res = requests.get(url, headers=HEADERS, verify=False)
        res.encoding = "euc-kr"
        soup = BeautifulSoup(res.text, "html.parser")
        content_div = soup.select_one("div#news_content")
        if content_div:
            content = content_div.get_text(separator="\n").strip()
        else:
            print(f"[!] ë³¸ë¬¸ íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤: {url}")
            content = ""
    except Exception as e:
        print(f"[ERROR] {url} - {e}")
    return content

# ===== ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ =====
def crawl_boannews(pages=2):
    all_articles = []
    for page in range(1, pages+1):
        print(f"\nðŸ“„ Fetching Page {page}")
        articles = get_article_list(page)
        for article in tqdm(articles, desc="ðŸ“¥ Downloading Articles"):
            content = get_article_content(article['uri'])
            article["content"] = content
            time.sleep(0.1)
        all_articles.extend(articles)
    return all_articles

# ===== KoBART ìš”ì•½ê¸° =====
def load_summarizer():
    tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")
    model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")
    return tokenizer, model

def summarize(text, tokenizer, model):
    input_ids = tokenizer.encode(text[:1024], return_tensors="pt", truncation=True)
    summary_ids = model.generate(
        input_ids,
        max_length=128,
        min_length=30,
        num_beams=4,
        length_penalty=1.2,
        no_repeat_ngram_size=3,
        early_stopping=True
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def remove_duplicate_sentences(text):
    sentences = list(dict.fromkeys(text.split('ë‹¤.')))  # ë¬¸ìž¥ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    return 'ë‹¤.'.join([s.strip() for s in sentences if s.strip()]) + 'ë‹¤.'

# ===== ë²•ë¥  ë¶„ë¥˜ê¸° ë¡œë”© =====
def load_law_classifier():
    tokenizer = BertTokenizer.from_pretrained("klue/roberta-base")
    model = BertForSequenceClassification.from_pretrained("klue/roberta-base", num_labels=len(LAW_LIST))
    model.eval()
    return tokenizer, model

# ===== ë©”ì¸ íŒŒì´í”„ë¼ì¸ =====
def main():
    print("ë³´ì•ˆë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ìš”ì•½/ë²•ë¥  ë¶„ì„ ì‹œìž‘")
    
    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
    news_data = crawl_boannews(pages=10)
    with open(RAW_SAVE, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    print(f"ë‰´ìŠ¤ ì›ë¬¸ {len(news_data)}ê±´ ì €ìž¥: {RAW_SAVE}")

    # 2. ìš”ì•½ê¸°/ë²•ë¥  ë¶„ë¥˜ê¸° ë¡œë“œ
    tokenizer_kobart, model_kobart = load_summarizer()
    law_tokenizer, law_model = load_law_classifier()

    # 3. ì²˜ë¦¬
    summarized = []
    print("\nìš”ì•½ ë° ë²•ë¥  ì¶”ì¶œ ì¤‘...")
    for article in tqdm(news_data):
        clean_text = remove_duplicate_sentences(article["content"])
        summary = summarize(clean_text, tokenizer_kobart, model_kobart)
        laws = classify_laws(article["content"], law_tokenizer, law_model)
        summarized.append({
            "title": article["title"],
            "date": article["date"],
            "uri": article["uri"],
            "summary": summary,
            "related_laws": laws
        })

    # 4. ì €ìž¥
    with open(SUMMARY_SAVE, "w", encoding="utf-8") as f:
        json.dump(summarized, f, ensure_ascii=False, indent=2)
    print(f"ìµœì¢… ìš”ì•½ ë° ë²•ë¥  ë¶„ì„ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {SUMMARY_SAVE}")

if __name__ == "__main__":
    main()