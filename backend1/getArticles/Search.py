import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm
import json
import time
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from datetime import datetime

# ===== ì„¤ì • =====
BASE_URL = "https://www.boannews.com"
START_URL = "https://www.boannews.com/search/news_hash.asp?find=%BB%E7%B0%C7%BB%E7%B0%ED"
HEADERS = {"User-Agent": "Mozilla/5.0"}
RAW_SAVE = "boan_news_raw.json"
SUMMARY_SAVE = "boan_news_summarized.json"

# ===== ì¶”ì • ê°€ëŠ¥í•œ í•œêµ­ ë²•ë¥  ëª©ë¡ =====
import torch

LAW_LIST = [
    "ê°œì¸ì •ë³´ë³´í˜¸ë²•",
    "ì •ë³´í†µì‹ ë§ë²•",
    "í†µì‹ ë¹„ë°€ë³´í˜¸ë²•",
    "ì •ë³´ë³´í˜¸ì‚°ì—…ë²•",
    "ì „ìê¸ˆìœµê±°ë˜ë²•",
    "ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)",
    "êµ­ê°€ì •ë³´í™”ê¸°ë³¸ë²•"
]

# ì˜ˆì‹œ ê°€ì¤‘ì¹˜ ì‚¬ì „ (í‚¤ì›Œë“œ: {ë²•ë¥ : ê°€ì¤‘ì¹˜})
KEYWORD_WEIGHTS = {
    # ê°œì¸ì •ë³´ë³´í˜¸ë²• ê´€ë ¨
    "ê°œì¸ì •ë³´ ìœ ì¶œ": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.25},
    "ê°œì¸ì •ë³´": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15},
    "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.2},
    "ê³ ê° ì •ë³´": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.2},
    "ì´ë¦„, ì£¼ì†Œ": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.1},
    "íœ´ëŒ€ì „í™”ë²ˆí˜¸": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15},
    "ì˜ë£Œ ì •ë³´": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.25},
    "ìœ ì¶œ ì‚¬ê³ ": {"ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15},
    "ìœ ì¶œ": {
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.20,
        "ì •ë³´í†µì‹ ë§ë²•": 0.10,
        "êµ­ê°€ì •ë³´í™”ê¸°ë³¸ë²•": 0.10
    },

    # ì •ë³´í†µì‹ ë§ë²• ê´€ë ¨
    "í•´í‚¹": {"ì •ë³´í†µì‹ ë§ë²•": 0.2, "í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.15},
    "ëœì„¬ì›¨ì–´": {"ì •ë³´í†µì‹ ë§ë²•": 0.25, "ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.2},
    "ë””ë„ìŠ¤": {"ì •ë³´í†µì‹ ë§ë²•": 0.2, "ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.15},
    "DDoS": {
        "ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.25,
        "ì •ë³´í†µì‹ ë§ë²•": 0.15,
        "ì •ë³´í†µì‹ ê¸°ë°˜ë³´í˜¸ë²•": 0.15
    },
    "ë°±ë„ì–´": {"ì •ë³´í†µì‹ ë§ë²•": 0.2},
    "ì›¹ì‰˜": {"ì •ë³´í†µì‹ ë§ë²•": 0.15},
    "ì•…ì„±ì½”ë“œ": {"ì •ë³´í†µì‹ ë§ë²•": 0.2},

    # í†µì‹ ë¹„ë°€ë³´í˜¸ë²• ê´€ë ¨
    "ê°ì²­": {"í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.25},
    "ë„ì²­": {"í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.25},
    "íŒ¨í‚· ë¶„ì„": {"í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.2},
    "íŠ¸ë˜í”½ ê°€ë¡œì±„ê¸°": {"í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.2},

    # ì „ìê¸ˆìœµê±°ë˜ë²• ê´€ë ¨
    "ì „ìê¸ˆìœµ": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.25},
    "ê¸ˆìœµ ì‚¬ê³ ": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.15},
    "í”¼ì‹±": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.25, "ì •ë³´í†µì‹ ë§ë²•": 0.15},
    "ìŠ¤ë¯¸ì‹±": {
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15,
        "ì „ìê¸ˆìœµê±°ë˜ë²•": 0.20,
        "í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.10
    },
    "ê³„ì¢Œ íƒˆì·¨": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.2},
    "OTP": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.15},
    "ê°€ìƒìì‚°": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.15},
    "ê²°ì œ": {
        "ì „ìê¸ˆìœµê±°ë˜ë²•": 0.20,
        "ì „ìë¬¸ì„œ ë° ì „ìê±°ë˜ ê¸°ë³¸ë²•": 0.15,
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.10
    },
    "ê¸ˆìœµ": {
        "ì „ìê¸ˆìœµê±°ë˜ë²•": 0.25,
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.10
    },
    "ê¸ˆìœµì •ë³´": {
        "ì „ìê¸ˆìœµê±°ë˜ë²•": 0.20,
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15,
        "ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸ì—ê´€í•œë²•ë¥ ": 0.20
    },

    # ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ) ê´€ë ¨
    "ì‚¬ì´ë²„ ê³µê²©": {"ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.2},
    "ë³´ì•ˆ ì·¨ì•½ì ": {"ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.15},
    "ì œë¡œë°ì´": {"ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.2},
    "APT": {"ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.2},
    "ì¹¨í•´ ì‚¬ê³ ": {"ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.2},
    "C2 ì„œë²„": {"ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.2},

    # êµ­ê°€ì •ë³´í™”ê¸°ë³¸ë²• ê´€ë ¨
    "êµ­ê°€ ì •ë³´": {"êµ­ê°€ì •ë³´í™”ê¸°ë³¸ë²•": 0.15},
    "í–‰ì •ë§": {"êµ­ê°€ì •ë³´í™”ê¸°ë³¸ë²•": 0.2},
    "ê³µê³µê¸°ê´€ ì‹œìŠ¤í…œ": {"êµ­ê°€ì •ë³´í™”ê¸°ë³¸ë²•": 0.2},

    # ì •ë³´ë³´í˜¸ì‚°ì—…ë²• ê´€ë ¨
    "ë³´ì•ˆ ì†”ë£¨ì…˜": {"ì •ë³´ë³´í˜¸ì‚°ì—…ë²•": 0.15},
    "ì •ë³´ë³´í˜¸ ì œí’ˆ": {"ì •ë³´ë³´í˜¸ì‚°ì—…ë²•": 0.15},
    "ë³´ì•ˆ ê¸°ì—…": {"ì •ë³´ë³´í˜¸ì‚°ì—…ë²•": 0.15},

    # ê¸°íƒ€ í‚¤ì›Œë“œ
    "ì¸ì¦ì„œ íƒˆì·¨": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.2, "ì •ë³´í†µì‹ ë§ë²•": 0.15},
    "ë””ì§€í„¸ ì¦ëª…ì„œ": {"ì „ìê¸ˆìœµê±°ë˜ë²•": 0.15},
    "ë”¥í˜ì´í¬": {
        "ì •ë³´í†µì‹ ë§ë²•": 0.20,
        "í†µì‹ ë¹„ë°€ë³´í˜¸ë²•": 0.10,
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": 0.15
    },
    "ë„ë°• ì‚¬ì´íŠ¸": {
        "ì •ë³´í†µì‹ ë§ë²•": 0.20,
        "ì‚¬ì´ë²„ë³´ì•ˆê¸°ë³¸ë²•(ì•ˆ)": 0.15
    }
}

def apply_keyword_weights(text, base_probs):
    scores = base_probs.clone()
    text_lower = text.lower()
    
    triggered_keywords = set()

    for keyword, law_weights in KEYWORD_WEIGHTS.items():
        if keyword in text_lower:
            triggered_keywords.add(keyword)

    for keyword in triggered_keywords:
        for law, weight in KEYWORD_WEIGHTS[keyword].items():
            if law in LAW_LIST:
                idx = LAW_LIST.index(law)
                scores[idx] += weight

    total = scores.sum().item()
    if total > 0:
        scores /= total
    else:
        scores = base_probs

    return scores

def classify_laws(text, tokenizer, model, top_k=1, threshold=0.1):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        base_probs = torch.softmax(outputs.logits, dim=1)[0]
    
    # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë°˜ì˜ ë° ì •ê·œí™”
    weighted_probs = apply_keyword_weights(text, base_probs)

    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë²•ë¥  í•˜ë‚˜ ì¶”ì¶œ
    top_index = weighted_probs.argmax().item()
    top_prob = weighted_probs[top_index]
    
    if top_prob > threshold:
        return LAW_LIST[top_index]
    else:
        return "ë¯¸ë¶„ë¥˜"

# ===== ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ =====
def get_article_list(page=1):
    url = f"{START_URL}&Page={page}"
    resp = requests.get(url, headers=HEADERS, verify=False)
    resp.encoding = 'euc-kr'
    soup = BeautifulSoup(resp.text, 'html.parser')
    articles = []
    for item in soup.select("div.news_list"):
        title = item.select_one("span.news_txt")
        date = item.select_one("span.news_writer")
        link = item.select_one("a[href^='/media/view.asp']")
        if title and date and link:
            # ì›ë³¸: "ê¸°ìëª… | 2024ë…„ 10ì›” 28ì¼ 13:39"
            raw_date = date.get_text(strip=True).split('|')[-1].strip()
            try:
                parsed_date = datetime.strptime(raw_date, "%Yë…„ %mì›” %dì¼ %H:%M")
                formatted_date = parsed_date.strftime("%Y.%m.%d")
            except ValueError:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                formatted_date = raw_date
            articles.append({
                "title": title.get_text(strip=True),
                "date": formatted_date,
                "uri": urljoin(BASE_URL, link['href']),
            })
    return articles

# ===== ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘ =====
def get_article_content(url):
    try:
        res = requests.get(url, headers=HEADERS, verify=False)
        res.encoding = "euc-kr"
        soup = BeautifulSoup(res.text, "html.parser")
        content_div = soup.select_one("div#news_content")
        if content_div:
            content = content_div.get_text(separator="\n").strip()
        else:
            print(f"[!] ë³¸ë¬¸ íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤: {url}")
            content = ""
    except Exception as e:
        print(f"[ERROR] {url} - {e}")
    return content

# ===== ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ =====
def crawl_boannews(pages=2):
    all_articles = []
    for page in range(1, pages+1):
        print(f"\nğŸ“„ Fetching Page {page}")
        articles = get_article_list(page)
        for article in tqdm(articles, desc="ğŸ“¥ Downloading Articles"):
            content = get_article_content(article['uri'])
            article["content"] = content
            time.sleep(0.1)
        all_articles.extend(articles)
    return all_articles

# ===== KoBART ìš”ì•½ê¸° =====
def load_summarizer():
    tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")
    model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")
    return tokenizer, model

def summarize(text, tokenizer, model):
    input_ids = tokenizer.encode(text[:1024], return_tensors="pt", truncation=True)
    summary_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# ===== ë²•ë¥  ë¶„ë¥˜ê¸° ë¡œë”© =====
def load_law_classifier():
    tokenizer = BertTokenizer.from_pretrained("klue/roberta-base")
    model = BertForSequenceClassification.from_pretrained("klue/roberta-base", num_labels=len(LAW_LIST))
    model.eval()
    return tokenizer, model

# ===== ë©”ì¸ íŒŒì´í”„ë¼ì¸ =====
def main():
    print("ğŸš€ ë³´ì•ˆë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ìš”ì•½/ë²•ë¥  ë¶„ì„ ì‹œì‘")
    
    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
    news_data = crawl_boannews(pages=4)
    with open(RAW_SAVE, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ë‰´ìŠ¤ ì›ë¬¸ {len(news_data)}ê±´ ì €ì¥: {RAW_SAVE}")

    # 2. ìš”ì•½ê¸°/ë²•ë¥  ë¶„ë¥˜ê¸° ë¡œë“œ
    tokenizer_kobart, model_kobart = load_summarizer()
    law_tokenizer, law_model = load_law_classifier()

    # 3. ì²˜ë¦¬
    summarized = []
    print("\nâœ ìš”ì•½ ë° ë²•ë¥  ì¶”ì¶œ ì¤‘...")
    for article in tqdm(news_data):
        summary = summarize(article["content"], tokenizer_kobart, model_kobart)
        laws = classify_laws(article["content"], law_tokenizer, law_model)
        summarized.append({
            "title": article["title"],
            "date": article["date"],
            "uri": article["uri"],
            "summary": summary,
            "related_laws": laws
        })

    # 4. ì €ì¥
    with open(SUMMARY_SAVE, "w", encoding="utf-8") as f:
        json.dump(summarized, f, ensure_ascii=False, indent=2)
    print(f"ğŸ‰ ìµœì¢… ìš”ì•½ ë° ë²•ë¥  ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {SUMMARY_SAVE}")

if __name__ == "__main__":
    main()